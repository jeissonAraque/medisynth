# -*- coding: utf-8 -*-
"""medisynth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmzSL5wWkIGzRxsFSzZ45ugPCxDJWrNr
"""

import torch
from diffusers import StableDiffusionPipeline, DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
import kagglehub
from peft import LoraConfig, get_peft_model
from datasets import load_dataset, Dataset, Features, Image as ImageFeature, ClassLabel
from torchvision import transforms
from torch.utils.data import DataLoader
import numpy as np
from PIL import Image
import os
import random
import shutil
from tqdm.auto import tqdm
import wandb
import json
import torch_fidelity
import matplotlib.pyplot as plt
from pathlib import Path

# Configuración
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Usando dispositivo: {device}")

# Configuración de entrenamiento
config = {
    "model_id": "runwayml/stable-diffusion-v1-5",
    "resolution": 512,
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-4,
    "max_train_steps": 500,
    "save_steps": 100,
    "mixed_precision": "fp16",
    "output_dir": "./medisyn-model",
    "seed": 42
}

# Ahora SÍ cargar el modelo (Paso 5 normal)
print("Cargando Stable Diffusion 1.5...")

tokenizer = CLIPTokenizer.from_pretrained(
    config["model_id"],
    subfolder="tokenizer"
)

text_encoder = CLIPTextModel.from_pretrained(
    config["model_id"],
    subfolder="text_encoder"
)

vae = AutoencoderKL.from_pretrained(
    config["model_id"],
    subfolder="vae"
)

unet = UNet2DConditionModel.from_pretrained(
    config["model_id"],
    subfolder="unet"
)

print("Modelos base cargados")

# ============================================================
# PASO 3: DESCARGA Y PREPARACIÓN DEL DATASET
# ============================================================
path = kagglehub.dataset_download("paultimothymooney/chest-xray-pneumonia")

print("Path to dataset files:", path)

def load_chest_xray_dataset(base_path, split="train", max_images=None):
    # Ajuste automático: si dentro hay otra carpeta 'chest_xray', úsala
    inner = os.path.join(base_path, "chest_xray")
    if os.path.exists(inner):
        base_path = inner

    split_path = os.path.join(base_path, split)

    if not os.path.exists(split_path):
        raise FileNotFoundError(f"No existe la carpeta: {split_path}")

    image_paths = []
    labels = []

    # Cargar imágenes NORMAL
    normal_dir = os.path.join(split_path, "NORMAL")
    pneumonia_dir = os.path.join(split_path, "PNEUMONIA")

    normal_files = [f for f in os.listdir(normal_dir) if f.endswith(('.jpeg', '.jpg', '.png'))]
    pneumonia_files = [f for f in os.listdir(pneumonia_dir) if f.endswith(('.jpeg', '.jpg', '.png'))]

    if max_images:
        normal_files = normal_files[:max_images]
        pneumonia_files = pneumonia_files[:max_images]

    for f in normal_files:
        image_paths.append(os.path.join(normal_dir, f))
        labels.append(0)

    for f in pneumonia_files:
        image_paths.append(os.path.join(pneumonia_dir, f))
        labels.append(1)

    # Convertir a Huggingface Dataset
    dataset_dict = {"image": image_paths, "label": labels}

    features = Features({
        "image": ImageFeature(),
        "label": ClassLabel(names=["normal", "pneumonia"])
    })

    return Dataset.from_dict(dataset_dict, features=features)

train_dataset = load_chest_xray_dataset(
    path,
    split="train",
    max_images=1000  # Cambiar a None para usar todas (~5000 imágenes)
)

print(f"\nDataset cargado exitosamente!")
print(f"   Total de imágenes: {len(train_dataset)}")
print(f"   Clases: {train_dataset.features['label'].names}")

# Ver ejemplo
print(f"\nEjemplo de datos:")
example = train_dataset[0]
print(f"   - Tipo de imagen: {type(example['image'])}")
print(f"   - Label: {example['label']} ({train_dataset.features['label'].names[example['label']]})")
print(f"   - Tamaño imagen: {example['image'].size}")

num_samples = 6
indices = random.sample(range(len(train_dataset)), num_samples)

plt.figure(figsize=(12, 6))

for i, idx in enumerate(indices):
    sample = train_dataset[idx]
    img = sample["image"]
    label = sample["label"]

    plt.subplot(2, 3, i + 1)
    plt.imshow(img, cmap="gray")
    plt.title(train_dataset.features["label"].names[label])
    plt.axis("off")

plt.tight_layout()
plt.show()

# ============================================================
# PASO 4: PREPROCESAMIENTO DE DATOS
# ============================================================

def preprocess_images(examples):
    """Preprocesa imágenes para Stable Diffusion"""

    # Convertir imágenes a RGB (por si vienen en escala de grises)
    if isinstance(examples["image"], list):
        images = [img.convert("RGB") if img.mode != "RGB" else img
                 for img in examples["image"]]
    else:
        img = examples["image"]
        images = [img.convert("RGB") if img.mode != "RGB" else img]

    transform = transforms.Compose([
        transforms.Resize((config["resolution"], config["resolution"])),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=5),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])  # Normalizar a [-1, 1]
    ])

    examples["pixel_values"] = [transform(img) for img in images]

    # Crear prompts descriptivos
    label_map = {
        0: "normal healthy chest x-ray, clear lungs, no infection",
        1: "chest x-ray showing pneumonia, lung infection, opacity in lungs"
    }

    labels = examples["label"] if isinstance(examples["label"], list) else [examples["label"]]

    examples["caption"] = [
        f"medical {label_map[label]}, high quality radiograph, diagnostic quality"
        for label in labels
    ]

    return examples

# Aplicar preprocesamiento
print("Preprocesando imágenes...")
train_dataset = train_dataset.map(
    preprocess_images,
    batched=True,
    batch_size=16,  # Procesar en lotes
    remove_columns=["image"]  # Eliminar imagen original para ahorrar memoria
)

train_dataset.set_format(type="torch", columns=["pixel_values", "caption"])

# Crear DataLoader
train_dataloader = DataLoader(
    train_dataset,
    batch_size=config["train_batch_size"],
    shuffle=True
)

print(f"   DataLoader creado con {len(train_dataloader)} batches")
print(f"   Batch size: {config['train_batch_size']}")
print(f"   Total steps por época: {len(train_dataloader)}")

# ============================================================
# PASO 5: CONFIGURAR LoRA
# ============================================================

print("\nConfigurando LoRA...")

lora_config = LoraConfig(
    r=8,                    # Rank de LoRA
    lora_alpha=16,          # Scaling factor
    target_modules=[        # Capas del UNet a adaptar
        "to_q",
        "to_v",
        "to_k",
        "to_out.0"
    ],
    lora_dropout=0.1,       # Regularización
)

# Aplicar LoRA al UNet
unet = get_peft_model(unet, lora_config)

print("\nResumen de parámetros:")
unet.print_trainable_parameters()

# ============================================================
# Mover modelos a GPU
# ============================================================

print("\n Moviendo modelos a GPU...")

vae = vae.to(device)
text_encoder = text_encoder.to(device)
unet = unet.to(device)

# Congelar VAE y text encoder (solo entrenar UNet con LoRA)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

print(" VAE y Text Encoder congelados")

# ============================================================
# Configurar scheduler de noise
# ============================================================

noise_scheduler = DDPMScheduler.from_pretrained(
    config["model_id"],
    subfolder="scheduler"
)

print("Noise scheduler cargado")

# Habilitar optimizaciones de memoria
unet.enable_gradient_checkpointing()

print("Gradient checkpointing habilitado")

print("\n" + "="*60)
print("MODELO CONFIGURADO Y LISTO PARA ENTRENAR")
print("="*60)

# ============================================================
# PASO 6: CONFIGURAR OPTIMIZADOR
# ============================================================

optimizer = torch.optim.AdamW(
    unet.parameters(),
    lr=config["learning_rate"],
    betas=(0.9, 0.999),
    weight_decay=1e-2,
    eps=1e-8
)

print(f"Optimizador configurado (LR: {config['learning_rate']})")

# ============================================================
# PASO 7: FUNCIÓN DE ENTRENAMIENTO
# ============================================================

def train_epoch(unet, vae, text_encoder, dataloader, optimizer, noise_scheduler, epoch_num):
    """Entrena una época"""
    unet.train()
    total_loss = 0

    progress_bar = tqdm(dataloader, desc=f"Época {epoch_num}")

    for step, batch in enumerate(progress_bar):
        # Obtener imágenes y captions
        pixel_values = batch["pixel_values"].to(device)
        captions = batch["caption"]

        # Encode imágenes a latent space
        with torch.no_grad():
            latents = vae.encode(pixel_values).latent_dist.sample()
            latents = latents * vae.config.scaling_factor

        # Agregar noise
        noise = torch.randn_like(latents)
        timesteps = torch.randint(
            0, noise_scheduler.config.num_train_timesteps,
            (latents.shape[0],),
            device=device
        ).long()
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        # Encode text prompts
        with torch.no_grad():
            text_inputs = tokenizer(
                captions,
                padding="max_length",
                max_length=tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt"
            )
            text_embeddings = text_encoder(
                text_inputs.input_ids.to(device)
            )[0]

        # Predicción de noise
        model_pred = unet(noisy_latents, timesteps, text_embeddings).sample

        # Calcular loss
        loss = torch.nn.functional.mse_loss(model_pred, noise)

        # Backward pass
        loss.backward()

        # Gradient accumulation
        if (step + 1) % config["gradient_accumulation_steps"] == 0:
            optimizer.step()
            optimizer.zero_grad()

        total_loss += loss.item()
        progress_bar.set_postfix({
            "loss": f"{loss.item():.4f}",
            "avg_loss": f"{total_loss/(step+1):.4f}"
        })

    return total_loss / len(dataloader)

print("Función de entrenamiento definida")

# ============================================================
# PASO 8: LOOP DE ENTRENAMIENTO
# ============================================================

print("\n" + "="*60)
print("INICIANDO ENTRENAMIENTO")
print("="*60)
os.makedirs(config["output_dir"], exist_ok=True)

# Calcular número de épocas
num_epochs = config["max_train_steps"] // len(train_dataloader) + 1

print(f"\nConfiguración:")
print(f"   - Total imágenes: {len(train_dataset)}")
print(f"   - Batch size: {config['train_batch_size']}")
print(f"   - Gradient accumulation: {config['gradient_accumulation_steps']}")
print(f"   - Effective batch size: {config['train_batch_size'] * config['gradient_accumulation_steps']}")
print(f"   - Steps por época: {len(train_dataloader)}")
print(f"   - Épocas a entrenar: {num_epochs}")
print(f"   - Total steps: {config['max_train_steps']}")
print(f"   - Learning rate: {config['learning_rate']}")

# Para guardar historial de loss
loss_history = []

# ENTRENAMIENTO
for epoch in range(num_epochs):
    print(f"\n{'='*60}")
    print(f"ÉPOCA {epoch + 1}/{num_epochs}")
    print(f"{'='*60}\n")

    avg_loss = train_epoch(
        unet, vae, text_encoder,
        train_dataloader, optimizer, noise_scheduler,
        epoch_num=epoch+1
    )

    loss_history.append(avg_loss)

    print(f"\nÉpoca {epoch+1} completada")
    print(f"   Loss promedio: {avg_loss:.4f}")

    # Guardar checkpoint cada 2 épocas
    if (epoch + 1) % 2 == 0:
        checkpoint_path = f"{config['output_dir']}/checkpoint-epoch-{epoch+1}"
        unet.save_pretrained(checkpoint_path)
        print(f"Checkpoint guardado en: {checkpoint_path}")

print("\n" + "="*60)
print("ENTRENAMIENTO COMPLETADO")
print("="*60)

# Guardar modelo final
final_path = f"{config['output_dir']}/final-lora"
unet.save_pretrained(final_path)
print(f"\nModelo final guardado en: {final_path}")

# Graficar loss
plt.figure(figsize=(10, 5))
plt.plot(loss_history, marker='o', linewidth=2, markersize=8)
plt.title('Training Loss por Época', fontsize=14, fontweight='bold')
plt.xlabel('Época', fontsize=12)
plt.ylabel('Loss (MSE)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.savefig(f"{config['output_dir']}/loss_curve.png", dpi=150, bbox_inches='tight')
plt.show()

print(f"\nGráfica de loss guardada en: {config['output_dir']}/loss_curve.png")

# Comprimir todo el modelo

shutil.make_archive('medisyn-model', 'zip', './medisyn-model')

print(f"\nModelo comprimido en: medisyn-model.zip")

# ============================================================
# PASO 9: PROBAR GENERACIÓN
# ============================================================

print("\n" + "="*60)
print("GENERANDO IMÁGENES DE PRUEBA")
print("="*60)


if unet.dtype != torch.float16:
    unet = unet.to(torch.float16)

# Cargar pipeline con tu modelo fine-tuned
print("\nCargando pipeline...")
pipeline = StableDiffusionPipeline.from_pretrained(
    config["model_id"],
    unet=unet,
    torch_dtype=torch.float16 
)
pipeline = pipeline.to(device)

# Prompts de prueba
test_prompts = [
    "medical chest x-ray showing pneumonia, lung infection, opacity in lungs, high quality radiograph",
    "medical normal healthy chest x-ray, clear lungs, no infection, diagnostic quality",
    "medical chest x-ray showing severe pneumonia, bilateral lung infection, medical imaging",
    "medical normal chest x-ray, healthy lungs, clear radiograph"
]

os.makedirs("./generated_samples", exist_ok=True)

print("\nGenerando imágenes...\n")

fig, axes = plt.subplots(2, 2, figsize=(12, 12))
axes = axes.flatten()

for i, prompt in enumerate(test_prompts):
    print(f"[{i+1}/{len(test_prompts)}] Generando: {prompt[:60]}...")

    image = pipeline(
        prompt,
        num_inference_steps=50,
        guidance_scale=7.5,
        generator=torch.Generator(device).manual_seed(42 + i)
    ).images[0]

    # Guardar
    image.save(f"./generated_samples/sample_{i+1}.png")

    # Mostrar
    axes[i].imshow(image)
    axes[i].set_title(f"Sample {i+1}\n{prompt[:40]}...", fontsize=10)
    axes[i].axis("off")

plt.tight_layout()
plt.savefig("./generated_samples/all_samples.png", dpi=150, bbox_inches='tight')
plt.show()

print(f"\nMuestras guardadas en: ./generated_samples/")
print(f"   - sample_1.png (neumonía)")
print(f"   - sample_2.png (normal)")
print(f"   - sample_3.png (neumonía severa)")
print(f"   - sample_4.png (normal)")

print("\n" + "="*60)
print("PROCESO COMPLETADO")
print("="*60)
print("\nArchivos generados:")
print(f"   - Modelo: {config['output_dir']}/final-lora/")
print(f"   - Loss curve: {config['output_dir']}/loss_curve.png")
print(f"   - Muestras: ./generated_samples/")

# ============================================================
# PASO 10: EVALUACIÓN CON MÉTRICAS
# ============================================================

print("\n" + "="*60)
print("EVALUANDO MODELO CON MÉTRICAS")
print("="*60)


# Generar más muestras para evaluación (mínimo 50)
print("\n Generando 50 muestras para evaluación...")

os.makedirs("./evaluation_samples", exist_ok=True)

# Mezcla de prompts normales y neumonía
eval_prompts = [
    "medical chest x-ray showing pneumonia, lung infection, opacity in lungs, high quality radiograph",
    "medical normal healthy chest x-ray, clear lungs, no infection, diagnostic quality",
] * 25  # 50 imágenes totales

for i, prompt in enumerate(eval_prompts):
    if i % 10 == 0:
        print(f"Generando imagen {i+1}/50...")

    image = pipeline(
        prompt,
        num_inference_steps=50,
        guidance_scale=7.5,
        generator=torch.Generator(device).manual_seed(i)
    ).images[0]

    if image.size != (config["resolution"], config["resolution"]):
        image = image.resize((config["resolution"], config["resolution"]), Image.LANCZOS)

    image.save(f"./evaluation_samples/eval_{i+1:03d}.png")

print("50 muestras generadas")

# Preparar carpeta con imágenes reales para comparación
print("\nPreparando imágenes reales de referencia...")

os.makedirs("./real_samples", exist_ok=True)

# Copiar 50 imágenes reales del dataset original y redimensionarlas

real_count = 0
# Obtener rutas del dataset
for split in ["train", "test"]:
    normal_dir = Path(path) / "chest_xray" / split / "NORMAL"
    pneumonia_dir = Path(path) / "chest_xray" / split / "PNEUMONIA"

    # Copiar y redimensionar normales
    for img_path in list(normal_dir.glob("*.jpeg")):
        if real_count >= 25: # Only copy up to 25 normal images for this split
            break
        img = Image.open(img_path).convert("RGB")
        img = img.resize((config["resolution"], config["resolution"]), Image.LANCZOS)
        img.save(f"./real_samples/real_{real_count:03d}.png")
        real_count += 1

    # Copiar y redimensionar neumonía
    for img_path in list(pneumonia_dir.glob("*.jpeg")):
        if real_count >= 50: # Total 50 images (25 normal + 25 pneumonia)
            break
        img = Image.open(img_path).convert("RGB")
        img = img.resize((config["resolution"], config["resolution"]), Image.LANCZOS)
        img.save(f"./real_samples/real_{real_count:03d}.png")
        real_count += 1

    if real_count >= 50:
        break

print(f"{real_count} imágenes reales preparadas")

# Calcular métricas FID e IS
print("\nCalculando métricas (esto toma 2-3 minutos)...")

metrics = torch_fidelity.calculate_metrics(
    input1="./evaluation_samples",  # Imágenes generadas
    input2="./real_samples",         # Imágenes reales
    cuda=True,
    isc=True,   # Inception Score
    fid=True,   # Fréchet Inception Distance
    verbose=False,
)

# Mostrar resultados
print("\n" + "="*60)
print("RESULTADOS DE EVALUACIÓN")
print("="*60)

fid_score = metrics['frechet_inception_distance']
is_mean = metrics['inception_score_mean']
is_std = metrics['inception_score_std']

print(f"\nFID Score: {fid_score:.2f}")
if fid_score < 50:
    print("   Excelente calidad (< 50)")
elif fid_score < 100:
    print("   Buena calidad (50-100)")
elif fid_score < 150:
    print("   Calidad aceptable (100-150)")
else:
    print("   Calidad mejorable (> 150)")

print(f"\nInception Score: {is_mean:.2f} \u00B1 {is_std:.2f}")
if is_mean > 5:
    print("   Excelente diversidad (> 5)")
elif is_mean > 3:
    print("   Buena diversidad (3-5)")
else:
    print("   Diversidad limitada (< 3)")

# Guardar métricas en JSON

results = {
    "experimento": "Experimento 1 - Baseline",
    "configuracion": {
        "total_imagenes": len(train_dataset),
        "max_train_steps": config["max_train_steps"],
        "learning_rate": config["learning_rate"],
        "batch_size": config["train_batch_size"],
    },
    "metricas": {
        "fid_score": float(fid_score),
        "inception_score_mean": float(is_mean),
        "inception_score_std": float(is_std),
    }
}

with open("./medisyn-model/evaluation_results.json", "w") as f:
    json.dump(results, f, indent=2)

print(f"\nResultados guardados en: ./medisyn-model/evaluation_results.json")

# Comparar visualmente real vs generado
print("\nComparación visual Real vs Generado:")

fig, axes = plt.subplots(2, 4, figsize=(16, 8))

# Fila 1: Reales
for i in range(4):
    real_img = Image.open(f"./real_samples/real_{i:03d}.png")
    axes[0, i].imshow(real_img, cmap='gray')
    axes[0, i].set_title(f"Real {i+1}", fontsize=12)
    axes[0, i].axis("off")

# Fila 2: Generadas
for i in range(4):
    gen_img = Image.open(f"./evaluation_samples/eval_{i+1:03d}.png") # FIX: Changed i to i+1
    axes[1, i].imshow(gen_img)
    axes[1, i].set_title(f"Generado {i+1}", fontsize=12)
    axes[1, i].axis("off")

plt.suptitle("Comparación: Imágenes Reales vs Generadas", fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig("./medisyn-model/comparison_real_vs_generated.png", dpi=150, bbox_inches='tight')
plt.show()

print("\nEvaluación completada")